{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf66686",
   "metadata": {},
   "source": [
    "CNN model for multi-class image classification of fashiom items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87342e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951bbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "# alternative syntax: df = keras.datasets.fashion_mnist\n",
    "(x_train_full,y_train_full),(x_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train_full[6]) # will output a matrix of 28x28 dimensions.\n",
    "# the grayscale images in the dataset are of 28x28 pixels.\n",
    "# 0 represents black & 255 represents white. \n",
    "# for each record, we have 28x28=784 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbe819",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_full[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a44c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\n",
    "               \"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle Boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4720e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names[y_train_full[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62c65fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) \n",
      " (60000,) \n",
      " (10000, 28, 28) \n",
      " (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_full.shape,\"\\n\",y_train_full.shape,\"\\n\",x_test.shape,\"\\n\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9275f1",
   "metadata": {},
   "source": [
    "## Data Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a0d26",
   "metadata": {},
   "source": [
    "This is the only change that we do during the pre-processing of CNN as compared to ANN. <br> In ANN we converted the 2D grayscale images into 1D array using flatten() method. <br> For CNN, we need a 3D array as input. We need height, width & channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aea72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full = x_train_full.reshape((60000,28,28,1)) # 1 stands for channel.\n",
    "x_test = x_test.reshape((10000,28,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe2827",
   "metadata": {},
   "source": [
    "## Data Normalisation\n",
    "so that the data dimensions are of approximately the same scale. normalisation means restricting the pixel entities btw 0 & 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c7a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide by float to get output in float\n",
    "x_train_norm = x_train_full/255.0\n",
    "x_test_norm = x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ed44d",
   "metadata": {},
   "source": [
    "## Train-Val-Test Split\n",
    "validation dataset is used for tuning the hyperparameters & to evaluate the model.<br>\n",
    "We have 60,000 records in train dataset & 10,000 records in test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be832c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_train = x_train_norm[:5000], x_train_norm[5000:]\n",
    "y_val, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "x_test = x_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3affa4",
   "metadata": {},
   "source": [
    "#### To delete previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1209acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c804e",
   "metadata": {},
   "source": [
    "## Model Structure/ Architecture & Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cad589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600286a",
   "metadata": {},
   "source": [
    "The dataset has observations in the form of 2D array of 28x28 pixels. <br>\n",
    "Step1: Use Keras Sequential API to build the NN. <br>\n",
    "Step2: Add a Convolutional Base & feed it into the input layer.<br>\n",
    "Step3: Convert 2D array into flat 1D array of 784 pixels.  <br>\n",
    "Step4: Use ReLU Activation for Hidden Layers. <br>\n",
    "Step5: Use Softmax Avtivation on Output Layer as it is a multiclass classification. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3606ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='valid', \n",
    "                             activation='relu', input_shape=(28,28,1)))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = \"sgd\",\n",
    "              loss = \"sparse_categorical_crossentropy\",\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347036a5",
   "metadata": {},
   "source": [
    "We are using sparse categorical crossentropy because our y_data is in form of 10 diff labels. <br>\n",
    "If we had binary labels (True/false, Yes/no), then we would use binary crossentropy. <br>\n",
    "If we had probabilities per class in y_data, then we would use categorical crossentropy.<br>\n",
    "SGD - Stochastic Gradient Descent. Keras will perform back propagation algo here. <br>\n",
    "Because, we are building a classifier, we are using Accuracy as metrics. <br>\n",
    "If we are building a regressor, we would use Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc02aae3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5408)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               1622700   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1654130 (6.31 MB)\n",
      "Trainable params: 1654130 (6.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d85e02",
   "metadata": {},
   "source": [
    "### Diagrammatic representation of model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2151ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fa7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92134dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba515e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d9d11",
   "metadata": {},
   "source": [
    "### Early Stopping Checkpoint & Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a44b494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"Fashion_Images_Model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ce596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae084c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=200, batch_size=64, \n",
    "                    validation_data=(x_val, y_val), \n",
    "                    callbacks = [checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf18b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e56486",
   "metadata": {},
   "source": [
    "the model has not converged yet because the val_accuracy is still going up & val_loss is still going low. so, we can run it for more epochs. if we call the fit() method again, Keras will continue to train the model where we previously left it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069db722",
   "metadata": {},
   "source": [
    "### Load the best model & evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e569694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.3313 - accuracy: 0.8809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3313138782978058, 0.8809000253677368]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"Fashion_Images_Model.h5\")\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9bf01",
   "metadata": {},
   "source": [
    "### Predict the class labels in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d495647f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.2 , 0.  , 0.77],\n",
       "       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = model.predict(x_test[:3])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a279eb6",
   "metadata": {},
   "source": [
    "x_test contains 3 records, so we will get output for 3 records & there are 10 values for each record representing the probability for corresponding class. <br>\n",
    "round(2) is to round off probability values to 2 decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f211333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "['Ankle Boot' 'Pullover' 'Trouser' 'Trouser' 'Shirt']\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(x_test[:5]),axis=1)\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c6ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, 1, 6], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52be59e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1efa5460c90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a48a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
